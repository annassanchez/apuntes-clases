{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proceso:\n",
    "1. separar variable predictoras, variable respuesta -> separamos entre train y test\n",
    "2. predecimos el precio de una nueva casa, que no está la variable respuesta pero si las predictoras\n",
    "<br>¿cómo sé si es correcta?\n",
    "\n",
    "se divide el dataframe en una distribución 70% / 30%; el 70% se entrena, el 30% restante, se testa.\n",
    "- el 70% / 80% busca patrones y genera una recta de regresión lineal\n",
    "- el 30% / 20% valida la predicción hecha -- y tienes los dos valores para comparar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se separa entre el df con las variables predictoras / variables respuesta\n",
    "- le pasas el método train_test_split(X, y, train_size = 0.8, random_state = 42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importante: checkear si te estás dejando las casas más caras / más grandes fuera --> checkeas con un describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "por como hace la división, parece que el modelo está un poco sesgado --> si comparas los valores, no son similares, hay bastante variación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ajustamos el modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el ajuste puede hacerse con sklearn\n",
    "\n",
    "para ello, se genera el objeto LinearRegression()\n",
    "\n",
    "haces el fit entre la variable predictora / variable respuesta\n",
    "\n",
    "haces la predicción con el dataframe de variables respuesta de test y con las variables respuesta de train. generas un dataframe con las variables respuesta predecida y la variable respuesta original, para poder ver como de bien predice el modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos a hacer una visualización de los residuos / errores -> es la diferencia entre el valor de las variable respuesta predicha y la variable respuesta original.\n",
    "\n",
    "lo ideal es estar lo más pegado posible a la línea de regresión -- eso quiere decir que están cercanos a la predicción. Si están alejados, consideramos que la predicción no se ha hecho con éxito, porque tienes el valor real en un punto, el predicho en otro y la distancia entre los dos será la distancia del punto en y a la línea de regresión."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "- MSE -- la más fácil de calcular pero es la más ambigua\n",
    "<br> es una especie de varianza de los errores\n",
    "- RMSE -- la más utilizada\n",
    "<br> es como la desviación estándar\n",
    "<br> penaliza más las desviaciones grandes\n",
    "<br> hace el fix de unidades (si predices km, te devuelve km en vez de km^2)\n",
    "<br> podemos considerarlo como una especie de margen de error\n",
    "<br> no tiene valor máx / mín, pero cuanto más cercano de 0 mejor\n",
    "- MAE\n",
    "<br> es en valor absoluto, por lo que no penaliza tanto esa distancia\n",
    "- R2 -- la cantidad (%) de variación en la variable respuesta explicada por las variables predictoras\n",
    "<br> has elegido las variables para hacer la predicción de forma correcta?\n",
    "<br> si el valor es muy bajo, tendrás que plantearte elegir otras variables predictoras\n",
    "<br> si el R = 1, el modelo está sobre entrenado -- interesa que este cercano, pero que no sea igual\n",
    "<br> - jack knife puede coger las variables y evaluar si una de ellas está ayudando a obtener el resultado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 cálculo de métricas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics.mean_squared_error -- MSE\n",
    "\n",
    "np.sqrt(metrics.mean_squared_error) -- RMSE --> según lo que se mida, da el valor\n",
    "- nos está dando 0.45 de rango de error\n",
    "    - si mido el precio de una chuchería, no sería aceptable\n",
    "    - si fuera el precio de una casa, sería un error muy aceptable\n",
    "\n",
    "sin embargo, este valor nos está dando el valor transformado --> tenemos que destransformarlo para saber el valor real\n",
    "\n",
    "metrics.r2_score() -- R^2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para ver en porcentaje lo que se ha equivocado la estimación:\n",
    "$$(rmse * 100) / (df['median_home_val'].max() - df['median_home_val'].min())$$\n",
    "el resultado es del 11.81% del valor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dentro de scipy, hay un método que puede hacer la inversa de la transformación efectuada en la normalización\n",
    "\n",
    "from scipy.special import inv_boxcos\n",
    "\n",
    "haciendo la inversa de ña media, vemos que el valor de la casa sería 880, por lo que sabemos que el valor puede llegar a estar +- 11.81%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿cómo saber si el modelo esta underfitted o overfitted?\n",
    "\n",
    "si acierta al 100% los valores del test -> el modelo está overfitteado\n",
    "\n",
    "- si las métricas son mejores en train que en el test -> overfitting\n",
    "  <br> se eliminan predictores\n",
    "  <br> aumentar el número de registros\n",
    "\n",
    "- si son similates -> el modelo está bien\n",
    "\n",
    "- si las métricas son mejores en el test que en el train -> underfitting --> los predictores elegidos no son buenos, el modelo acierta por azar\n",
    "  <br> se añaden más predictores\n",
    "  <br> aumentar el número de registros\n",
    "\n",
    "ejemplo: si tienes un R2 de 0.89 pero un RMSE 20000 para estimar precios de coche, pese a que el valor de R2 es muy bueno, el RMSE no es aceptable. Es mucho mejor, reevaluar las variables que se están teniendo en cuenta en el modelo, y a partir de eso volver a hacer la estimación, aunque eso perjudique la R2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "1. df completo\n",
    "2. difividimos entre: \n",
    "   - X: predictoras\n",
    "    - y: variable respuesta\n",
    "3. dividimos entre: \n",
    "   - train\n",
    "   - test\n",
    "4. entreno con .fit al modelo\n",
    "5. hago las predicciones\n",
    "   - train\n",
    "   - test\n",
    "     - me fijo en las métricas del test\n",
    "6. miramos las métricas del test -- examinar si el modelo es bueno\n",
    "   - compararemos train/test -- para ver si hay over / underfitting\n",
    "     - cuando las métricas del train son mejores que las del test + R2 = 1 + RMSE = 0 -> overfitting\n",
    "         <br> quitamos columnas / aumentando la muestra\n",
    "     - cuando las métricas del test son mejores que las del train -> underfitting\n",
    "  <br> añadiendo columnas / aumentando la muestra\n",
    "7. las métricas que hay:\n",
    "   - MSE -- varianza\n",
    "   - MRSE -- stdev --> el mejor, cuanto más cercano a 0, mejor\n",
    "   - MAE -- valor absoluto\n",
    "   - R2 -- % de la variación de la variable respuesta que está explicada por la variable predictora --> el mejor, cuanto más cercano a 1, mejor; ojo con el overfitting.\n",
    "8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
